{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "sys.path.append(\"..\")\n",
    "from qtable import QAgent, SimpleMaze\n",
    "from qnn import QAgentNN, wrap_as_tensor3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maze = SimpleMaze()\n",
    "agent = QAgentNN(dim_state=(1, 1, 2), range_state=((((0, 3),(0, 4)),),),actions=maze.actions,\n",
    "                 learning_rate=0.01, reward_scaling=100,\n",
    "                 freeze_period=100,\n",
    "                 alpha=0.5, gamma=0.5, explore_strategy='fixed_epsilon', epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze and agent initialized!\n",
      "1483 1114 4 7 645 34 2 3 5 2 2 3 291 303 3 99 92 2 3 89 3 3 4 34 3 4 46 3 3 6 4 5 4 41 3 3 4 4 5 4 3 6 3 3 4 4 3 3 4 3 3 4 5 3 3 4 2 5 5 5 5 5 4 3 4 4 3 5 3 4 3 2 2 2 2 2 5 3 3 2 4 4 4 4 3 3 5 4 3 4 4 5 4 2 4 4 4 6 5 5 \n",
      "100 8386 4494 1.86604361371\n",
      "3 4 5 5 4 3 5 3 4 3 4 4 3 2 3 4 3 4 3 4 5 4 3 3 2 5 3 4 3 4 3 3 4 4 3 4 4 3 3 2 3 4 4 3 4 2 3 5 5 4 4 4 2 3 2 4 3 2 2 4 3 3 5 3 3 5 5 4 2 3 2 4 2 3 3 4 3 2 4 4 2 4 4 4 4 3 4 3 4 4 5 3 3 4 3 3 4 2 3 3 \n",
      "200 10000 245 40.8163265306\n",
      "5 4 3 6 3 2 4 2 2 5 3 4 3 3 3 2 2 5 5 4 5 2 2 2 4 5 3 3 2 3 2 5 2 4 3 5 2 2 3 4 3 5 3 4 3 2 4 4 2 4 2 3 2 2 2 3 2 4 4 3 3 4 5 2 3 4 3 5 6 4 3 3 2 3 5 5 3 3 3 4 5 4 3 2 4 3 2 4 4 5 3 3 2 4 5 3 5 4 4 3 \n",
      "300 9999 239 41.8368200837\n",
      "4 4 3 2 2 3 3 5 3 3 3 4 2 3 5 4 2 5 3 5 3 3 2 2 3 2 2 6 3 4 6 3 4 3 5 4 3 3 4 4 3 4 4 2 4 4 2 3 3 3 4 2 2 3 3 3 2 4 3 3 3 4 2 4 3 3 3 2 3 3 2 4 3 4 5 4 3 4 5 3 2 3 4 4 5 3 4 4 3 3 2 3 2 3 3 3 5 3 2 3 \n",
      "400 9999 229 43.6637554585\n",
      "4 5 4 2 4 3 4 3 5 4 4 2 4 4 3 4 3 4 4 3 3 5 2 4 4 4 3 5 3 3 2 3 4 4 5 3 3 4 2 3 3 4 3 4 3 4 3 5 4 3 2 3 3 5 2 2 3 3 2 3 3 2 3 5 4 4 3 3 4 3 3 3 2 2 3 3 2 2 4 2 2 4 3 2 3 3 2 4 4 4 4 4 4 4 3 3 4 3 2 2 \n",
      "500 9999 230 43.4739130435\n",
      "6 5 2 5 4 4 3 4 3 2 3 3 3 2 4 5 2 3 4 2 5 3 4 6 3 4 2 3 3 2 3 4 2 2 3 4 4 3 2 3 5 3 3 2 3 4 4 2 3 4 4 3 4 4 3 3 3 3 3 3 3 3 4 3 3 3 3 4 2 3 3 4 2 2 2 3 3 3 4 3 3 4 3 4 3 2 3 3 6 5 4 3 4 4 5 3 4 2 2 3 \n",
      "600 10000 230 43.4782608696\n",
      "3 3 4 2 3 3 2 3 3 3 4 3 4 2 2 4 4 2 2 3 4 5 3 3 4 4 2 2 3 3 4 4 3 4 4 4 4 3 4 3 5 2 3 2 4 2 4 5 3 3 2 3 4 3 3 5 2 3 4 4 4 4 2 4 4 3 3 3 3 3 3 4 4 4 4 3 3 3 2 4 5 3 3 3 2 2 3 3 4 4 4 3 3 4 4 5 2 4 5 5 \n",
      "700 10000 233 42.9184549356\n",
      "3 3 4 4 2 3 4 3 3 4 4 5 3 4 4 3 4 4 3 3 4 3 2 4 3 3 5 3 2"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-728eb0bb50c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrap_as_tensor3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mnew_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaze\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_observation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwrap_as_tensor3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m# print action,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# print new_observation,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/admin-326/Data/ipython-notebook/dqn4wirelesscontrol/qlearning/qnn.py\u001b[0m in \u001b[0;36mreinforce\u001b[1;34m(self, current_observation, reward)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreeze_counter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFREEZE_PERIOD\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_table_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreeze_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreeze_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/admin-326/Data/ipython-notebook/dqn4wirelesscontrol/qlearning/qnn.py\u001b[0m in \u001b[0;36mupdate_table_\u001b[1;34m(self, last_state, last_action, reward, current_state)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_table_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun_train_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrescale_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrescale_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 911\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maze = SimpleMaze()\n",
    "agent = QAgentNN(dim_state=(1, 1, 2), range_state=((((0, 3),(0, 4)),),),actions=maze.actions,\n",
    "                 learning_rate=0.01, reward_scaling=100, batch_size=100,\n",
    "                 freeze_period=100, memory_size=1000,\n",
    "                 alpha=0.5, gamma=0.5, explore_strategy='epsilon', epsilon=0.02)\n",
    "print \"Maze and agent initialized!\"\n",
    "\n",
    "# logging\n",
    "path = deque()  # path in this episode\n",
    "episode_reward_rates = []\n",
    "num_episodes = 0\n",
    "cum_reward = 0\n",
    "cum_steps = 0\n",
    "\n",
    "# repeatedly run episodes\n",
    "while True:\n",
    "    maze.reset()\n",
    "    new_observation = maze.observe()\n",
    "    agent.reset()\n",
    "\n",
    "    path.clear()\n",
    "    path.append(new_observation)\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    episode_loss = 0\n",
    "\n",
    "    # interact and reinforce repeatedly\n",
    "    while not maze.isfinished():\n",
    "        action = agent.act(wrap_as_tensor3(new_observation))\n",
    "        new_observation, reward = maze.interact(action)\n",
    "        loss = agent.reinforce(current_observation=wrap_as_tensor3(new_observation), reward=reward)\n",
    "        # print action,\n",
    "        # print new_observation,\n",
    "        path.append(new_observation)\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        episode_loss += loss if loss else 0\n",
    "    print len(path),\n",
    "    # print \"{:.3f}\".format(episode_loss),\n",
    "    # print \"\"\n",
    "    cum_steps += episode_steps\n",
    "    cum_reward += episode_reward\n",
    "    num_episodes += 1\n",
    "    episode_reward_rates.append(episode_reward / episode_steps)\n",
    "    if num_episodes % 100 == 0:\n",
    "        print \"\"\n",
    "        print num_episodes, cum_reward, cum_steps, 1.0 * cum_reward / cum_steps #, path\n",
    "        cum_reward = 0\n",
    "        cum_steps = 0\n",
    "win = 50\n",
    "s = pd.rolling_mean(pd.Series([0]*win+episode_reward_rates), window=win, min_periods=1)\n",
    "s.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
